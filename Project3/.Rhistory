within_var = apply(within_var_matrix, 2, mean)
tot_var = within_var + between_var + between_var/(length(df_list)) # within_var + between_var * corrected_degrees_of_freedom
pooled_se = sqrt(tot_var)
result = data.frame(cbind(pooled_coefs, pooled_se))
# Add rownames, but remove the second element which is an intercept always equal to 0
rownames(result) = rownames(lasso_coefs)[-2]
# Plot the results #####
plot(result, xlim = c(-0.21, 0.26), xlab = "Pooled coefficients", ylab = "Total standard error")
lines(c(0, -0.3), c(0, 0.3/1.96))
lines(c(0, 0.3), c(0, 0.3/1.96))
text(pooled_se ~ pooled_coefs, labels = rownames(result), data = result, pos = 4)
result[result == 0] = "."
result
##### ONLY DO THIS IF THE DATA IS NOT SPLIT INTO TRAINING AND TEST DATA #####
# We do not need to split the data into training and test set
# But we remove the code variable
df_train = data.TPA.full[, -1]
head(df_train)
# Here we will split the training data set into the respectiv 40 sets and make a list of them
# data frames are 0 indexed (for some reason)
n = dim(df_train)[1]/40
df_list = list(df_train[0:n, ])
for (i in 1:39){
df_list[[i+1]] = df_train[(n*i + 1):(n*(i+1)), ]
}
# Set seed for reproducability
set.seed(135)
# We shuffle the observation to make sure the folds are not split according to some standardised randomisation
shuffle = sample(1:n, n)
# Make a grid of lambdas
lambda_grid = c(seq(from = 0, to = 0.5, 0.005))
# K gives the type of CV
K = 5
# a list to store values for the different models
cv_results = matrix(nrow = K, ncol = length(lambda_grid))
l = 0
for (lambda in lambda_grid){
l = l +1
cv_error = c()
# K-fold cross validation to choose the approriate lambda
for (k in 1:K){
# Create fold k from the shuffled values
fold_k = shuffle[((k-1)*n/K + 1):(k*n/K)]
# Now we do lasso for each data set on the specific lambda and store the coefficients
validation_errors = c()
for (df in df_list){
x = model.matrix(tpa15_a1_cpm~., df[-fold_k, ])
y = df[-fold_k, ]$tpa15_a1_cpm
fit_lasso = glmnet(x, y, alpha = 1) # alpha = 1 gives lasso and is the default of glmnet()
lasso_coefs = coef(fit_lasso, s = lambda)
###### validate her on the k-th fold, but with which of the 40 data sets? a different each time for variation?
x = df[fold_k, -dim(df)[2]] # remove the response
y = df[fold_k, ]$tpa15_a1_cpm
# We convert x into a matrix and add 1s on a new first column because we have an intercept
x = cbind(rep(1, nrow(x)),data.matrix(x))
y_hat = x %*% lasso_coefs[-2]
# Append the validation error/MSE to the vector
validation_errors = c(validation_errors, sqrt(apply((y_hat - y)^2, 2, mean))) # Need to take the mean of this outside the loop?
}
# Combine the validation error
combined_mse = mean(validation_errors) # Mean MSE
# Using MSE will also punish the lambda which gives variable results, which is a nice side effects?
cv_error = c(cv_error, combined_mse)
# To see the progress of the algorithm
print(k)
}
# To see the progress of the algorithm
print(lambda)
cv_results[, l] = cv_error
}
# Make a data frame to use lambdas as column names
val_error_df = data.frame(cv_results)
colnames(val_error_df) = lambda_grid
# mean of the errors across the folds
comb_val_err = apply(val_error_df, 2, mean)
# Gives the optimum lambda (which.min(comb_val_err) gives the position in lambda_grid)
lambda_best = lambda_grid[which.min(comb_val_err)]
best_cv_err = min(comb_val_err)
##### plot the optimum lambda #####
plot(lambda_grid, comb_val_err, type = 'b', ylab = "Validation error", xlab = bquote(lambda))
points(lambda_best, best_cv_err, pch = 20, col = 'red')
# Initialise a matrix with ncol = # number of columns in each data set - 1(the respons) + 1(intercept)
coef_matrix = matrix(nrow = length(df_list), ncol = dim(df_list[[1]])[2])
i = 0
# Now we do lasso for each data set on the specific lambda and store the coefficients
validation_errors = c()
for (df in df_list){
i = i + 1
x = model.matrix(tpa15_a1_cpm~., df)
y = df$tpa15_a1_cpm
fit_lasso = glmnet(x, y, alpha = 1) # alpha = 1 gives lasso and is the default of glmnet()
lasso_coefs = coef(fit_lasso, s = lambda_best)
# append the the coefficients to the i-th row,
# why do we get 2 intercepts lasso_coefs have length 38 not 37 as ncol in coef_matrix
# We remove the second element, which is a second intercept always equal 0 (for different lambdas)
coef_matrix[i, ] = t(lasso_coefs[-2])
}
# average the coefficients from the 40 data sets
pooled_coefs = apply(coef_matrix, 2, mean)
# Get the between-standard-error for the coefficients
between_var = apply(coef_matrix, 2, var)
# The number of bootstrap samples to estimate coefficeints
boot_length = 100
# Matrix to store values
within_var_matrix = matrix(nrow = length(df_list), ncol = length(pooled_coefs))
i = 0
for (df in df_list){
i = i + 1
df_temp = df # I do this in case df_temp needs to be changed
# a matrix to store the coefficients in
boot_coef_matrix = matrix(nrow = boot_length, ncol = length(pooled_coefs))
for (boot_iteration in 1:boot_length){
# We get sample bootstrap indices
boot_indices = sample(dim(df_temp)[2], dim(df_temp)[2], replace = TRUE)
# bootstrap sample
boot_sample = df_temp[boot_indices, ]
# make x using the bootstrap samples
x = model.matrix(tpa15_a1_cpm~., boot_sample)
# y i responses for the bootstrap sample
y = boot_sample$tpa15_a1_cpm
# do lasso on x and y
fit_lasso = glmnet(x, y, alpha = 1)
# store coefficient values in the matrix
boot_coef_matrix[boot_iteration, ] = as.vector(coef(fit_lasso, s = lambda_best))[-2]
}
within_var_matrix[i, ] = apply(boot_coef_matrix, 2, var)
}
# Now we take the mean of the bootstrap variances
within_var = apply(within_var_matrix, 2, mean)
tot_var = within_var + between_var + between_var/(length(df_list)) # within_var + between_var * corrected_degrees_of_freedom
pooled_se = sqrt(tot_var)
result = data.frame(cbind(pooled_coefs, pooled_se))
# Add rownames, but remove the second element which is an intercept always equal to 0
rownames(result) = rownames(lasso_coefs)[-2]
# Plot the results #####
plot(result, xlim = c(-0.21, 0.26), xlab = "Pooled coefficients", ylab = "Total standard error")
lines(c(0, -0.3), c(0, 0.3/1.96))
lines(c(0, 0.3), c(0, 0.3/1.96))
text(pooled_se ~ pooled_coefs, labels = rownames(result), data = result, pos = 4)
result[result == 0] = "."
result
install.packages("here")
?pf
library(magrittr)
library(grid)
library(gridExtra)
library(lattice)
library(tidyverse)
library(purrr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(bookdown)
library(readr)
library(glue)
library(latex2exp)
seed = 123321
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=7, fig.height=4, fig.align = "center")
bilirubin = read_tsv("bilirubin.txt")
bilirubin = data.frame(bilirubin)
head(bilirubin)
boxplot(log(meas) ~ pers, data = bilirubin, xlab = "Individual", ylab = "Log concentration")
bilirubin_lm = lm(log(meas)~pers,data = bilirubin)
summary(bilirubin_lm)
lm_summary = summary(bilirubin_lm)
Fval = lm_summary$fstatistic[1]
cat(Fval, qf(0.9, 2, 26), Fval > qf(0.9, 2, 26))
cat(Fval, qf(0.95, 2, 26), Fval > qf(0.95, 2, 26))
cat(Fval, qf(0.99, 2, 26), Fval > qf(0.99, 2, 26))
permTest = function(pers, meas){
# Shuffle the meas vector
randomised_meas = sample(meas, length(meas))
# Fit linear model with meas on log scale
perm_lm = lm(log(randomised_meas)~ pers)
# Get the summary
lm_summary = summary(perm_lm)
# Exstract the f statistic from the summary
return(lm_summary$fstatistic[1])
}
pt = permTest(bilirubin$pers, bilirubin$meas)
pf(q, 2, 26)
permTest = function(pers, meas){
# Shuffle the meas vector
randomised_meas = sample(meas, length(meas))
# Fit linear model with meas on log scale
perm_lm = lm(log(randomised_meas)~ pers)
# Get the summary
lm_summary = summary(perm_lm)
# Exstract the f statistic from the summary
return(lm_summary$fstatistic[1])
}
pt = permTest(bilirubin$pers, bilirubin$meas)
pf(pt, 2, 26)
for (i in 1:5){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
pf(pt, 2, 26)
for (i in 1:50){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
head(pf(pt, 2, 26))
for (i in 1:50){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
head(pf(pt, 2, 26))
max(pt)
min(pt)
head(pf(pt, 2, 26, lower.tail = FALSE))
max(pt)
min(pt)
p_values = pf(pt, 2, 26, lower.tail = FALSE)
head(p_values)
max(p_values)
min(p_values)
for (i in 1:500){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
pt = c()
for (i in 1:500){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
p_values = pf(pt, 2, 26, lower.tail = FALSE)
head(p_values)
max(p_values); min(p_values)
hist(p_values)
pt = c()
for (i in 1:1000){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
p_values = pf(pt, 2, 26, lower.tail = FALSE)
head(p_values)
max(p_values); min(p_values)
hist(p_values)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(bookdown)
library(readr)
library(glue)
library(latex2exp)
seed = 123321
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=7, fig.height=4, fig.align = "center")
bilirubin = read_tsv("bilirubin.txt")
bilirubin =data.frame(bilirubin)
head(bilirubin)
boxplot(log(meas)~ pers, data = bilirubin, xlab = "Individual", ylab = "Concentration")
bilirubin_lm = lm(log(meas)~pers,data = bilirubin)
lm_summary = summary(bilirubin_lm)
Fval = lm_summary$fstatistic[1]
cat(Fval, qf(0.9, 2, 26), Fval > qf(0.9, 2, 26))
cat(Fval, qf(0.95, 2, 26), Fval > qf(0.95, 2, 26))
cat(Fval, qf(0.99, 2, 26), Fval > qf(0.99, 2, 26))
permTest = function(pers, meas){
# Shuffle the meas vector
randomised_meas = sample(meas, length(meas))
# Fit linear model with meas on log scale
perm_lm = lm(log(randomised_meas)~ pers)
# Get the summary
lm_summary = summary(perm_lm)
# Exstract the f statistic from the summary
return(lm_summary$fstatistic[1])
}
pt = c()
for (i in 1:1000){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
p_values = pf(pt, 2, 26, lower.tail = FALSE)
head(p_values)
cat(max(p_values), min(p_values))
hist(p_values)
pt = c()
for (i in 1:1000){
pt = c(pt, permTest(bilirubin$pers, bilirubin$meas))
}
p_values = pf(pt, 2, 26, lower.tail = FALSE)
glue("This is the first five p-values we generate {head(p_values)}")
glue("Here we see that the largest p-value = {cat(max(p_values)}
and the smallest = {min(p_values)}")
set.seed(seed = seed)
f_vals = c()
for (i in 1:999){
f_vals = c(f_vals, permTest(bilirubin$pers, bilirubin$meas))
}
hist(f_vals, breaks = 30, freq = F, xlab = "F-statistics",
main = "Histogram of 999 sampled F-statistics")
p_value = length(f_vals[f_vals > Fval])/length(f_vals)
p_value
install.packages("mlr")
################### #
# Attach packages ----
################### #
# hola
library(tidyverse)
library(tidymodels)
library(xgboost)
library(readxl)
library(writexl)
library(data.table)
library(mlr)
houses_raw <- read_excel("temp/houses.xlsx")
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
houses_raw
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
houses
houses_raw <- read_excel("temp/houses.xlsx")
houses_raw
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
houses
################### #
# Split data using the rsample package ----
# Set seed in order to make the analysis reproducible
################### #
set.seed(42)
split <- initial_split(houses, prop = 3/4)
train_raw <- training(split)
test_raw  <- testing(split)
train <- train_raw %>%
select(-kommune_name, -id)
test <- test_raw %>%
select(-kommune_name, -id)
model <- boost_tree(trees = 350) %>%
set_mode("regression") %>%
set_engine("xgboost") %>%
fit(tot_price ~ ., data = train)
model_preds <-
predict(model, test) %>%
bind_cols(test_raw) %>%
rename(estimate     = .pred,
truth        = tot_price) %>%
mutate(abs_dev      = abs(truth - estimate),
abs_dev_perc = abs_dev/truth)
mape(model_preds, truth, estimate)
xgb.importance(model = model$fit) %>%
xgb.plot.importance()
str(test)
table(is.na(train))
> sapply(train, function(x) sum(is.na(x))/length(x))*100
sapply(train, function(x) sum(is.na(x))/length(x))*100
nfold = 5
params = list(booster = "gbtree", objective = "binary:logistic",
eta=0.3, gamma=0, max_depth=6, min_child_weight=1,
subsample=1, colsample_bytree=1)
xgbcv <- xgb.cv( params = params, data = train, nrounds = 100,
nfold = nfold, showsd = T, stratified = T,
print.every.n = 10, early.stop.round = 20, maximize = F)
str(train)
houses = matrix(haouses)
houses = matrix(houses)
str(houses)
houses = as.matrix(houses)
str(houses)
houses = xgb.Dmatrix(houses)
houses = xgb.DMatrix(houses)
houses_raw <- read_excel("temp/houses.xlsx")
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
houses
str(houses)
houses = xgb.DMatrix(matrix(unlist(houses)))
houses
houses
houses_raw <- read_excel("temp/houses.xlsx")
houses_raw
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
install.packages("IRkernel")
IRkernel::installspec(user = FALSE)
################### #
# Attach packages ----
################### #
# hola
library(tidyverse)
library(tidymodels)
library(xgboost)
library(readxl)
library(writexl)
houses_raw <- read_excel("temp/houses.xlsx")
houses <- houses_raw %>%
select(id, sqm, expense, tot_price, lat, lng, kommune_name) %>%
mutate(kommune_factor = fct_lump_n(kommune_name, 220) %>% as.integer())
View(houses)
?gbt.train
agtboost::gbt.train
agtboost::predict
?agtboost::predict()
?agtboost::gbt.importance()
packageVersion('rmarkdown')
rmarkdown::pandoc_version()
library(magrittr)
library(grid)
library(gridExtra)
library(lattice)
library(tidyverse)
library(purrr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(bookdown)
library(readr)
library(glue)
library(latex2exp)
library(formatR)
seed = 123321
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=50), message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=7, fig.height=4, fig.align = "center")
EM = function(z, u, lambda_0 = 2, lambda_1 = 10, threshold = 10^(-7)){
n = length(u)
lambda_matrix = matrix(nrow = 1, ncol = 2)
lambda_matrix[1, ] = c(lambda_0, lambda_1)
diff = 1000
while (diff > threshold) {
# Expectation step
exp_log_like = n*(log(lambda_0) + log(lambda_1)) -
lambda_0 * sum(u*z + (1-u)*(1/lambda_0 - z/(exp(lambda_0*z) - 1)))
# Maximisation step
lambda_0_new = n/(sum(u*z + (1-u)*(1/lambda_0 - z/(exp(lambda_0*z) - 1))))
lambda_1_new = n/(sum((1 - u)*z + u*(1/lambda_1 - z/(exp(lambda_1*z) - 1))))
# Add the lambdas to the output matrix
lambda_matrix = rbind(lambda_matrix, c(lambda_0_new, lambda_1_new))
# The loop ends when the maximum absolute difference in lambda_0 and lambda_1 is lower than the threshold
diff = max(abs(lambda_0 - lambda_0_new), abs(lambda_1 - lambda_1_new))
# rename the variables
lambda_0 = lambda_0_new
lambda_1 = lambda_1_new
}
return(list("lambda_matrix" = lambda_matrix, "lambda_0" = lambda_0, "lambda_1" = lambda_1))
}
library(magrittr)
library(grid)
library(gridExtra)
library(lattice)
library(tidyverse)
library(purrr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(bookdown)
library(readr)
library(glue)
library(latex2exp)
library(formatR)
seed = 123321
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=50), message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=7, fig.height=4, fig.align = "center")
z = read_tsv("z.txt")
u = read_tsv("u.txt")
# Convert them to numeric vectores
z = data.frame(z)[, 1]
u = data.frame(u)[, 1]
expected_log_likelihood = function(parameters) {
lambda_0 = parameters[1]
lambda_1 = parameters[2]
length(u)*(log(lambda_0) + log(lambda_1)) -
lambda_0 * sum(u*z + (1-u)*(1/lambda_0 - z/(exp(lambda_0*z) - 1)))
}
expected_log_likelihood(c(lambda_0, lambda_1))
expected_log_likelihood = function(parameters) {
lambda_0 = parameters[1]
lambda_1 = parameters[2]
length(u)*(log(lambda_0) + log(lambda_1)) -
lambda_0 * sum(u*z + (1-u)*(1/lambda_0 - z/(exp(lambda_0*z) - 1)))
}
lambda_0 = 2
lambda_1 = 10
expected_log_likelihood(c(lambda_0, lambda_1))
EM = function(z, u, lambda_0 = 2, lambda_1 = 10, iter = 20, threshold = 10^(-7)){
n = length(u)
lambda_matrix = matrix(nrow = 1, ncol = 2)
lambda_matrix[1, ] = c(lambda_0, lambda_1)
diff = 1000
while (diff > threshold) {
# Expectation step
#exp_log_like = n*(log(lambda_0) + log(lambda_1)) -
#lambda_0 * sum(u*z + (1-u)*(1/lambda_0 - z/(exp(lambda_0*z) - 1)))
# Maximisation step
lambda_0_new = n/(sum(u*z + (1-u)*(1/lambda_0 - z/(exp(lambda_0*z) - 1))))
lambda_1_new = n/(sum((1 - u)*z + u*(1/lambda_1 - z/(exp(lambda_1*z) - 1))))
# Add the lambdas to the output matrix
lambda_matrix = rbind(lambda_matrix, c(lambda_0_new, lambda_1_new))
# The loop ends when the maximum absolute difference in lambda_0 and lambda_1 is lower than the threshold
diff = max(abs(lambda_0 - lambda_0_new), abs(lambda_1 - lambda_1_new))
# rename the variables
lambda_0 = lambda_0_new
lambda_1 = lambda_1_new
}
return(list("lambda_matrix" = lambda_matrix, "lambda_0" = lambda_0, "lambda_1" = lambda_1))
}
set.seed(seed)
em_output = EM(z, u)
glue("The EM algorithm outputs lambda_0 = {round(em_output$lambda_0, 4)}
and lambda_1 = {round(em_output$lambda_1, 4)}")
diffs_lambda_0 = abs(diff(em_output$lambda_matrix[, 1]))
diffs_lambda_1 = abs(diff(em_output$lambda_matrix[, 2]))
plot(1:length(diffs_lambda_0), diffs_lambda_0, type = 'b', xlab = "Iteration",
ylab = expression(abs(lambda[i]^{t+1} - lambda[i]^{t})), col = "dark orange")
lines(1:length(diffs_lambda_1), diffs_lambda_1, type = 'b', col = "dark green")
legend("topright", inset = 0.02,
legend = c(expression(lambda[0]), expression(lambda[1])),
col = c("dark orange", "dark green"),
lty = 1, cex=1.5)
